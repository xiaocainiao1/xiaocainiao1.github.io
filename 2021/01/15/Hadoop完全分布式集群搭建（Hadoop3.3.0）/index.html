<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Hadoop完全分布式集群搭建（Hadoop3.3.0）简介最近在接触大数据相关的项目，项目需求：Hadoop3.3.0 数据库 + CentOs7 环境部署，本着自以为很简单的心态开始部署，结果由于网上教程过于老旧，再加上各位大佬发的帖子基本一知半解，导致部署过程中查找资料不下20篇，踩过的坑不计其数，本着技术共享的心态特将此次实际部署过程完整的发布出来，以便大家少走弯路，保证如果细心看完之后能">
<meta property="og:type" content="article">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/2021/01/15/Hadoop%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%EF%BC%88Hadoop3.3.0%EF%BC%89/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Hadoop完全分布式集群搭建（Hadoop3.3.0）简介最近在接触大数据相关的项目，项目需求：Hadoop3.3.0 数据库 + CentOs7 环境部署，本着自以为很简单的心态开始部署，结果由于网上教程过于老旧，再加上各位大佬发的帖子基本一知半解，导致部署过程中查找资料不下20篇，踩过的坑不计其数，本着技术共享的心态特将此次实际部署过程完整的发布出来，以便大家少走弯路，保证如果细心看完之后能">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://pics5.baidu.com/feed/dbb44aed2e738bd44c23a071320cdfd1267ff9fb.jpeg?token=d42b09b4c2f344c0acd225b0e39698d6">
<meta property="og:image" content="https://pics1.baidu.com/feed/30adcbef76094b363787dc41334b24de8c109d90.jpeg?token=0b1ba1ab25026e58377f2effe596a432">
<meta property="og:image" content="https://pics7.baidu.com/feed/562c11dfa9ec8a13932d2de36484c988a0ecc009.jpeg?token=6f69b8554f324027bce5c950d1377d49">
<meta property="og:image" content="https://pics3.baidu.com/feed/2fdda3cc7cd98d10364e6276b1b8e0097aec9069.png?token=8b722d42b94d26ab4195441f617ba88f">
<meta property="og:image" content="https://pics6.baidu.com/feed/11385343fbf2b2113c327a6e59073d3f0cd78e05.jpeg?token=f0dbdf43bebc5f94f001a2b63f0b6b53">
<meta property="og:image" content="https://pics0.baidu.com/feed/b8014a90f603738d3750448f209ce856f919ec90.jpeg?token=028c34d8aad1d2be224af7f1a11f7825">
<meta property="og:image" content="https://pics2.baidu.com/feed/14ce36d3d539b60076771dba79d76d2dc75cb794.jpeg?token=4a39ad23d452c64790c90224fc2056ce">
<meta property="og:image" content="c:/Users/Hasee/AppData/Roaming/Typora/typora-user-images/image-20210115102346130.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20190426151030284.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI1NTQyODc5,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20190426151104960.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI1NTQyODc5,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://pics6.baidu.com/feed/314e251f95cad1c88f206fc6ecb93f0ecb3d51c2.jpeg?token=b5fa84e23d3c752917fde19977af63c8">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20190426151434502.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI1NTQyODc5,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20190426151450178.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI1NTQyODc5,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20190426151500367.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI1NTQyODc5,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20190426151514490.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI1NTQyODc5,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20190426151729686.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI1NTQyODc5,size_16,color_FFFFFF,t_70">
<meta property="article:published_time" content="2021-01-15T01:20:28.265Z">
<meta property="article:modified_time" content="2021-01-15T05:56:40.551Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pics5.baidu.com/feed/dbb44aed2e738bd44c23a071320cdfd1267ff9fb.jpeg?token=d42b09b4c2f344c0acd225b0e39698d6">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 5.4.2"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Hadoop完全分布式集群搭建（Hadoop3.3.0）" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2021/01/15/Hadoop%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%EF%BC%88Hadoop3.3.0%EF%BC%89/" class="article-date">
  <time datetime="2021-01-15T01:20:28.265Z" itemprop="datePublished">2021-01-15</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Hadoop完全分布式集群搭建（Hadoop3-3-0）"><a href="#Hadoop完全分布式集群搭建（Hadoop3-3-0）" class="headerlink" title="Hadoop完全分布式集群搭建（Hadoop3.3.0）"></a>Hadoop完全分布式集群搭建（Hadoop3.3.0）</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a><strong>简介</strong></h2><p>最近在接触大数据相关的项目，项目需求：Hadoop3.3.0 数据库 + CentOs7 环境部署，本着自以为很简单的心态开始部署，结果由于网上教程过于老旧，再加上各位大佬发的帖子基本一知半解，导致部署过程中查找资料不下20篇，踩过的坑不计其数，本着技术共享的心态特将此次实际部署过程完整的发布出来，以便大家少走弯路，保证如果细心看完之后能够通过本篇文章一站式部署成功，不用在东奔西走去搜集各种资料和踩坑。</p>
<p>想要完全掌握一个新架构，就需要知其然再知其所以然，这样以便日后进行修改和维护，所以本文将从零开始直至完全部署完毕，细节和重点将给各位画出来。</p>
<p><strong>一、Hadoop数据库是什么？</strong></p>
<p>Hadoop数据库是一个分布式框架数据库，属于非关系型数据库（更详细的请自行科普）</p>
<p><strong>二、Hadoop分布式数据库和关系型数据库区别</strong></p>
<p><strong>1、关系型数据库特点：</strong>结构化存储，一对多的层次型特性，数据类型基于字符串。表现形式类似于Execel表，数据以每个Excel表的样式存储在数据库中。</p>
<p><strong>2、非关系型数据库特点：</strong></p>
<p>非关系型数据库通常以字典数据类型存储，没有对应关系表。Hadoop数据库利用name node字段记录文件被分散存储的属性值，利用date node字段存储文件分散路径，表现形式类似于Windows系统注册表和系统文件的对应关系。</p>
<p><strong>三、Hadoop特性</strong></p>
<p>Hadoop就是为了解决海量数据的存储与运算问题，它所存储的数据类型是消息和文件，消息用来记录文件的存放属性，文件会被分布存储到各个Hadoop文件系统中，并且在整个群集中形成多个副本。</p>
<p>Hadoop数据库分为多个功能，这些功能在一个安装包中，每台服务器安装其中一个功能，进行多台安装。</p>
<p>通常用一台name node，用来存放文件属性，多台data node 用来存放文件块。 Hadoop分布式数据库读写原理。</p>
<p><strong>四、Hadoop运行环境</strong></p>
<p>Linux系统，Java JDK8以上开发环境组件。</p>
<p><strong>五、Hadoop分布式数据库读写原理</strong></p>
<p>1、由接口收发数据</p>
<p>2、由分布式调度平台将任务分为存储分布式和运算分布式进行任务下发。</p>
<p>3、存储任务：目标数据会分分割成若干个小份，以文件的形式散落在各个Hadoop分布式文件系统中，并且每个小块会在整个群集中存储多个副本，当需要取出时进行文件聚合。</p>
<p>4、运算任务: 一个任务会被分发到多台服务器中并行计算，快速计算出结果。</p>
<p><strong>六、Hadoop基本架构</strong></p>
<p>name node字段: 用来记录消息，也就是 分布式文件块大小，分割成几个块等。</p>
<p>data node: 用来存储文件路径和文件。</p>
<p><img src="https://pics5.baidu.com/feed/dbb44aed2e738bd44c23a071320cdfd1267ff9fb.jpeg?token=d42b09b4c2f344c0acd225b0e39698d6" alt="img"></p>
<p><strong>七、Hadoop交互方式介绍</strong></p>
<p>Hadoop交互分为命令行模式和交互式界面：</p>
<p><strong>1、命令行模式</strong>: Hadoop通过Linux终端方式连接，操作命令和Linux系统命令相差 不多。</p>
<p><strong>2、界面交互</strong>: 可以通过web浏览器方式访问Hadoop节点服务器。</p>
<h2 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h2><p>1、准备工作：</p>
<p>（1）准备Linux服务器：虚拟机或物理机都行，用来搭建Hadoop分布式数据库。</p>
<p>（2）下载JDK 8 。</p>
<p>（3）下载 Hadoop3.3.0 for Linux 安装包。</p>
<p>（4）推荐到官网下载，或自行搜索下载，这里推荐<a target="_blank" rel="noopener" href="http://mirrors.hust.edu.cn/apache/hadoop/common/hadoop-3.3.0/">华中科技大学镜像</a>下载</p>
<h2 id="开始部署Hadoop分布式数据库"><a href="#开始部署Hadoop分布式数据库" class="headerlink" title="开始部署Hadoop分布式数据库"></a>开始部署Hadoop分布式数据库</h2><h4 id="1、修改MAC地址（非虚拟机无需）"><a href="#1、修改MAC地址（非虚拟机无需）" class="headerlink" title="1、修改MAC地址（非虚拟机无需）"></a><strong>1、修改MAC地址</strong>（非虚拟机无需）</h4><p>克隆的虚拟机，为避免Mac地址冲突，需要关闭虚拟机，重新生成MAC地址。</p>
<p><img src="https://pics1.baidu.com/feed/30adcbef76094b363787dc41334b24de8c109d90.jpeg?token=0b1ba1ab25026e58377f2effe596a432" alt="img">客户端连接</p>
<h4 id="2、修改主机名："><a href="#2、修改主机名：" class="headerlink" title="2、修改主机名："></a><strong>2、修改主机名：</strong></h4><p>例如：hdp01 、hdp02、hdp0…</p>
<p>(1) 修改显示主机名：hostnamectl set-hostname hdp01</p>
<p>(2) 修改网络主机名：</p>
<p>vi /etc/sysconfig/network ~ 编辑配置文件</p>
<p>HOSTNAME=hdp01 ~ 将网络主机名改为hdp01</p>
<h4 id="3、修改静态IP（仅本地物理或虚拟机需要改）："><a href="#3、修改静态IP（仅本地物理或虚拟机需要改）：" class="headerlink" title="3、修改静态IP（仅本地物理或虚拟机需要改）："></a><strong>3、修改静态IP（仅本地物理或虚拟机需要改）：</strong></h4><p>将服务器修改成静态IP：如果是云服务器不需要改ip地址，否则会断网。</p>
<p>（1）编辑配置文件：vi /etc/sysconfig/network-scripts/ifcfg-ens33 添加下面的 代码。（vi编辑器 i 进入编辑模式、ESC退出编辑模式、:wq 保存退出）</p>
<p>IPADDR=192.168.1.10</p>
<p>GATEWAY=192.168.1.1</p>
<p>NETMASK=255.255.255.0</p>
<p>DNS1=114.114.114.114</p>
<p><img src="https://pics7.baidu.com/feed/562c11dfa9ec8a13932d2de36484c988a0ecc009.jpeg?token=6f69b8554f324027bce5c950d1377d49" alt="img"></p>
<p>（2）重启网络：service network restart</p>
<h4 id="4、增加域名映射："><a href="#4、增加域名映射：" class="headerlink" title="4、增加域名映射："></a><strong>4、增加域名映射：</strong></h4><p>在没有域的情况下，主机之间能通过主机名来互相访问，这就是修改hosts文件的作用，编辑hosts文件：vi /etc/hosts 输入要映射的主机名和IP地址（IP地址改成你当前主机的IP）。</p>
<p>192.168.1.11 hdp01</p>
<p>192.168.1.12 hdp02</p>
<h4 id="5、安装scp和-yum-所有主机"><a href="#5、安装scp和-yum-所有主机" class="headerlink" title="5、安装scp和 yum(所有主机)"></a><strong>5、安装scp和 yum(所有主机)</strong></h4><p>（1）先安装scp：如果能正常装上，yum update 就不用做。</p>
<p>scp工具：Linux下文件拷贝的基本工具，建议安装，可以使用命令安装。</p>
<p>yum install openssh-clients -y</p>
<p>（2）安装yum工具：后面用来安装系统组件、其他软件的必备工具，可以使用命令 安装或本地安装两种方式， yum update 。</p>
<h4 id="6、重点！卸载系统自带JAVA（每台主机）"><a href="#6、重点！卸载系统自带JAVA（每台主机）" class="headerlink" title="6、重点！卸载系统自带JAVA（每台主机）"></a><strong>6、重点！卸载系统自带JAVA（每台主机）</strong></h4><p>因系统自带JAVA版本可能比较老，而此时又新增高版本JAVA，可能在下面的配置过程中产生冲突，需提前检查，如果有则卸载。</p>
<p>（1）查看当前已安装的JAVA：rpm -qa|grep jdk</p>
<p>（2）使用rpm -e –nodeps命令将这些查询出来的全卸载掉：</p>
<p>rpm -e –nodeps java-1.8.0-openjdk-1.8.0.242.b08-1.el7.x86_64</p>
<p>（3）查看当前jdk版本：java -version</p>
<p>（4）刷新环境变量：source /etc/profile</p>
<h4 id="7、安装JDK、配置JAVA环境变量（每台主机）"><a href="#7、安装JDK、配置JAVA环境变量（每台主机）" class="headerlink" title="7、安装JDK、配置JAVA环境变量（每台主机）"></a><strong>7、安装JDK、配置JAVA环境变量（每台主机）</strong></h4><p>以上都配置好之后，就可以通过远程方式，拿终端连接Linux主机，一切都在终端上完成，推荐使用 SecureCRT 工具。</p>
<p>（1）上传JDK包到Linux：先在根路径下创建一个Hadoop的目录，用来存放java包和Hadoop包。</p>
<p>mkdir /hadoop3</p>
<p>（2）上传JDK包：将JDK压缩包上传到/Hadoop目录下备用）。</p>
<p>cd /hadoop3</p>
<p>put D:\jdk-14.0.8_linux-x64_bin.tar.gz</p>
<p><img src="https://pics3.baidu.com/feed/2fdda3cc7cd98d10364e6276b1b8e0097aec9069.png?token=8b722d42b94d26ab4195441f617ba88f" alt="img"></p>
<p>（3）配置JAVA环境变量</p>
<p>解压JDK包到当前目录：tar -zxvf D:\jdk-14.0.2_linux-x64_bin.tar.gz</p>
<p><img src="https://pics6.baidu.com/feed/11385343fbf2b2113c327a6e59073d3f0cd78e05.jpeg?token=f0dbdf43bebc5f94f001a2b63f0b6b53" alt="img"></p>
<p>编辑环境变量配置文件：vi /etc/profile</p>
<p>export JAVA_HOME=/hadoop/jdk-14.0.2 （你的解压完jdk后的那个主目录）</p>
<p>拼接环境变量：export PATH=$PATH**:**$JAVA_HOME/bin</p>
<p><img src="https://pics0.baidu.com/feed/b8014a90f603738d3750448f209ce856f919ec90.jpeg?token=028c34d8aad1d2be224af7f1a11f7825" alt="img">截图仅供参考实际目录以你当前jdk为准</p>
<p>测试JAVA环境变量：配置完环境变量后测试，保证配置是正确的，再向下进行：直接输入 JAVA。</p>
<p><img src="https://pics2.baidu.com/feed/14ce36d3d539b60076771dba79d76d2dc75cb794.jpeg?token=4a39ad23d452c64790c90224fc2056ce" alt="img"></p>
<h4 id="8、配置免密登陆"><a href="#8、配置免密登陆" class="headerlink" title="8、配置免密登陆 !"></a><strong>8、配置免密登陆 !</strong></h4><p>配置从A主机到其他所有主机的免密登陆：</p>
<p>（1）在A主机生成密钥：</p>
<p>。使用root用户登陆系统</p>
<p>。 生成密钥：ssh-keygen (生成的密钥文件在/root/.ssh下 id_rsa 、id_rsa.pub)</p>
<p>（2）配置自己对自己的免密登陆：ssh-copy-id hdp01 （hdp01就是你当前第一台的主机名）</p>
<p>（3）将密钥文件拷贝到其他所有主机上：ssh-copy-id hdp02 回车</p>
<p>。。。 根据提示输入 yes ，根据提示输入对方主机的 root 密码，敲回车。</p>
<p>成功的提示：Now try logging into the machine, with “ssh ‘hdp02’ “,</p>
<p>and check in: .ssh/authorized_keys</p>
<p>（4）如果还需要配置其他免密登陆的主机继续：ssh-copy-id hdp03 ….</p>
<p>（5）配置完免密登陆后执行一次：ssh hdp01 、ssh hdp02</p>
<h4 id="9、关闭防火墙"><a href="#9、关闭防火墙" class="headerlink" title="9、关闭防火墙"></a><strong>9、关闭防火墙</strong></h4><p>搭建的过程中先将防火墙关闭，或者放行端口：</p>
<p>关闭防火墙命令：systemctl stop firewalld.service</p>
<p>开启防火墙命令：systemctl start firewalld.service</p>
<h4 id="10、hadoop3-3-0安装搭建"><a href="#10、hadoop3-3-0安装搭建" class="headerlink" title="10、hadoop3.3.0安装搭建"></a>10、hadoop3.3.0安装搭建</h4><p>这里我下载hadoop的版本是3.3.0，下载地址：<br><a target="_blank" rel="noopener" href="http://mirrors.hust.edu.cn/apache/hadoop/common/hadoop-3.3.0/">http://mirrors.hust.edu.cn/apache/hadoop/common/hadoop-3.3.0/</a></p>
<p><img src="C:\Users\Hasee\AppData\Roaming\Typora\typora-user-images\image-20210115102346130.png"></p>
<ol>
<li><p>切换到想要下载的目录，使用命令下载到虚拟机上，前提是要确保虚拟机能够连接上外网。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget http://mirrors.hust.edu.cn/apache/hadoop/common/hadoop-3.3.0/hadoop-3.3.0.tar.gz</span><br></pre></td></tr></table></figure></li>
<li><p>创建一个存放hadoop文件的目录</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /usr/local/hadoop</span><br></pre></td></tr></table></figure></li>
<li><p>切换到hadoop安装包路径，解压hadoop安装包到创建的目录</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf hadoop-3.1.1.tar.gz -C /usr/local/hadoop</span><br></pre></td></tr></table></figure></li>
<li><p>开始配置hadoop环境变量</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/profile</span><br></pre></td></tr></table></figure>

<p>进入编辑状态，配置下列信息：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_HOME=/usr/local/hadoop</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</span><br></pre></td></tr></table></figure>

<p>让环境变量立即生效</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure>

<p>检验hadoop 环境变量配置是否生效</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop version</span><br></pre></td></tr></table></figure>

<p><img src="https://img-blog.csdnimg.cn/20190426151030284.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI1NTQyODc5,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
</li>
<li><p>开始进行hadoop的集群文件配置，在配置前，我们只需要对一台虚拟机（节点）配置就可以，其余3台虚拟机（3个节点）只需要将配置好的那台虚拟机复制过来就可以了。</p>
<h4 id="配置文件目录-（在安装目录-usr-local-hadoop-etc-hadoop-下）"><a href="#配置文件目录-（在安装目录-usr-local-hadoop-etc-hadoop-下）" class="headerlink" title="配置文件目录 （在安装目录 /usr/local/hadoop/etc/hadoop/ 下）"></a>配置文件目录 （在安装目录 /usr/local/hadoop/etc/hadoop/ 下）</h4><ul>
<li><strong><a target="_blank" rel="noopener" href="http://hadoop-env.sh/">hadoop-env.sh</a></strong></li>
<li><strong>core-site.xml</strong></li>
<li><strong>hdfs-site.xml</strong></li>
<li><strong>mapred-site.xml</strong></li>
<li><strong>yarn-site.xml</strong></li>
<li><strong>workers</strong></li>
</ul>
</li>
</ol>
<h3 id="hadoop文件配置"><a href="#hadoop文件配置" class="headerlink" title="hadoop文件配置"></a>hadoop文件配置</h3><ul>
<li><p>首先，配置hadoop-env.sh文件，如图所示：<br><img src="https://img-blog.csdnimg.cn/20190426151104960.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI1NTQyODc5,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>找到已经注释了”export JAVA_HOME”的代码行，按照上图写入对应的变量。</p>
<p><strong>参考代码</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_211</span><br><span class="line">export HADOOP_PID_DIR=/usr/local/hadoop/hadoop_data/tmp/pids</span><br><span class="line">export HDFS_NAMENODE_USER=root</span><br><span class="line">export HDFS_DATANODE_USER=root</span><br><span class="line">export HDFS_SECONDARYNAMENODE_USER=root</span><br><span class="line">export YARN_RESOURCEMANAGER_USER=root</span><br><span class="line">export YARN_NODEMANAGER_USER=root</span><br></pre></td></tr></table></figure>

<h4 id="配置注意事项"><a href="#配置注意事项" class="headerlink" title="配置注意事项"></a>配置注意事项</h4><p>1.JAVA_HOME的路径一定要填写绝对路径！</p>
<p>2.HADOOP_PID_DIR的值可以先填上去，后面再去创建，创建的时候最好放在hadoop安装目录下，而且只需要创建到tmp文件夹就行，pids会自动生成，方便拷贝到其他节点（虚拟机）上</p>
<p>3.其余的KEY对应的值都是root，按照上图配置就好了</p>
</li>
<li><p>配置core-site.xml文件，如图所示：<br><img src="https://pics6.baidu.com/feed/314e251f95cad1c88f206fc6ecb93f0ecb3d51c2.jpeg?token=b5fa84e23d3c752917fde19977af63c8" alt="img"></p>
<p><strong>参考代码</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">	        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">	        &lt;value&gt;hdfs://hadoop1:9000&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">	        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">	        &lt;value&gt;/usr/local/hadoop/hadoop_data/tmp&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line">	&lt;!-- 当前用户全设置成root --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;root&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<h4 id="配置说明"><a href="#配置说明" class="headerlink" title="配置说明"></a>配置说明</h4><ul>
<li><strong>fs.defaultFS</strong>  hadoop系统需要手动指定默认文件系统为HDFS，并且在众多服务器中指定其中一台作为name node服务器</li>
<li><strong>hadoop.tmp.dir</strong> 配置hadoop存储临时文件的路径</li>
<li><strong>我们需要手动创建这个目录，上面的步骤已经说明了</strong></li>
</ul>
</li>
<li><p>配置hdfs-site.xml文件，如图所示：<br><img src="https://img-blog.csdnimg.cn/20190426151434502.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI1NTQyODc5,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>默认情况下name node和data node配置文件存储在tmp目录下，但这是危险的，必须通过手动修改配置文件的存储路径。第一台name node服务器需要同时配置name node和data node配置文件路径，其他服务器只需要改data node路径即可；通过修改 hdfs-site.xml 配置文件实现。</p>
<p><strong>参考代码</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">            &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;3&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">            &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;/hadoop3/namenode&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">            &lt;name&gt;dfs.datanode.name.dir&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;/hadoop3/datanode&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">            &lt;name&gt;dfs.namenode.http-address&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;hadoop1:50070&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">            &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;hadoop2:50090&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<h4 id="配置说明-1"><a href="#配置说明-1" class="headerlink" title="配置说明"></a>配置说明</h4><ul>
<li><strong>dfs.replication</strong> 设置文件的备份数量</li>
<li><strong>dfs.namenode.http-address</strong> 设置哪台虚拟机作为namenode节点</li>
<li><strong>dfs.namenode.secondary.http-address</strong> 设置哪台虚拟机作为冷备份namenode节点，用于辅助namenode</li>
</ul>
</li>
<li><p>配置mapred-site.xml文件，如图所示：<br><img src="https://img-blog.csdnimg.cn/20190426151450178.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI1NTQyODc5,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p><strong>参考代码</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">            &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<h4 id="配置说明-2"><a href="#配置说明-2" class="headerlink" title="配置说明"></a>配置说明</h4><ul>
<li><strong><a target="_blank" rel="noopener" href="http://mapreduce.framework.name/">mapreduce.framework.name</a></strong> 配置yarn来进行任务调度</li>
</ul>
</li>
<li><p>配置yarn-site.xml文件，如图所示：<br><img src="https://img-blog.csdnimg.cn/20190426151500367.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI1NTQyODc5,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p><strong>参考代码</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">	&lt;!-- Site specific YARN configuration properties --&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">	        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">	        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line">	</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">	        &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line">	        &lt;value&gt;hadoop2&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">	&lt;property&gt;</span><br><span class="line">    		&lt;name&gt;yarn.application.classpath&lt;/name&gt;</span><br><span class="line"> 			&lt;value&gt;/usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/*:/usr/local/hadoop/share/hadoop/common/*:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/*:/usr/local/hadoop/share/hadoop/hdfs/*:/usr/local/hadoop/share/hadoop/mapreduce/lib/*:/usr/local/hadoop/share/hadoop/mapreduce/*:/usr/local/hadoop/share/hadoop/yarn:/usr/local/hadoop/share/hadoop/yarn/lib/*:/usr/local/hadoop/share/hadoop/yarn/*&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<h4 id="配置说明-3"><a href="#配置说明-3" class="headerlink" title="配置说明"></a>配置说明</h4><ul>
<li><strong>yarn.resourcemanager.hostname</strong> 配置yarn启动的主机名，也就是说配置在哪台虚拟机上就在那台虚拟机上进行启动</li>
<li><strong>yarn.application.classpath</strong> 配置yarn执行任务调度的类路径，如果不配置，yarn虽然可以启动，但执行不了mapreduce。执行<strong>hadoop classpath</strong>命令,将出现的类路径放在<value>标签里<br><strong>(注：其他机器启动是没有效果的）</strong></li>
</ul>
</li>
<li><p>配置workers文件，如图所示：<br><img src="https://img-blog.csdnimg.cn/20190426151514490.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI1NTQyODc5,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p><strong>参考代码</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop1</span><br><span class="line">hadoop2</span><br><span class="line">hadoop3</span><br></pre></td></tr></table></figure>

<h4 id="配置说明-4"><a href="#配置说明-4" class="headerlink" title="配置说明"></a>配置说明</h4><ul>
<li><strong>workers</strong> 配置datanode工作的机器，而datanode主要是用来存放数据文件的，我这里配置了4台，可能你会疑惑，hadoop1怎么也可以配置进去。其实，hadoop1我既配置作为namenode，它也可以充当datanode。当然你也可以选择不将namenode节点也配置进来。</li>
</ul>
</li>
<li><p>以上文件配置完毕之后，我们就可以用这台配置好的节点进行远程拷贝了。</p>
<p><strong>1</strong>. 首先，我们需要前期准备工作，先把其余有安装过hadoop的机器进行清除，在安装路径统一的情况下，这样方便我们前期的清理。<br>假设我配置好的文件放在hadoop1节点上，我将执行如下步骤：</p>
<ul>
<li><p>免密登陆到节点hadoop2,执行清理hadoop安装目录（其他机器没有安装的朋友可以不用操作此步骤）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ssh hadoop2 </span><br><span class="line">rm -rf /usr/local/hadoop</span><br><span class="line">exit</span><br></pre></td></tr></table></figure></li>
</ul>
<p>同理，按照上述步骤清理其余机器hadoop的安装目录</p>
<p><strong>2</strong>. 然后，我们进行远程拷贝操作，有两份需要拷贝，一份是环境变量，一份是hadoop的安装文件</p>
<ul>
<li><p>我们在当前配置过的节点hadoop1上执行远程拷贝命令</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">```</span><br><span class="line">scp -r /usr/local/hadoop hadoop2:/usr/local</span><br><span class="line">scp -r /etc/profile hadoop2:/etc</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- 拷贝完成之后，我们远程到hadoop2上，让环境变量立即生效</span><br><span class="line"></span><br></pre></td></tr></table></figure>
   <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ssh hadoop2</span><br><span class="line">source /etc/profile</span><br><span class="line">hadoop version</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">  如果控制台显示出hadoop版本信息，则环境变量生效，执行退出 `exit`</span><br><span class="line"></span><br><span class="line">- 同理，按照上述步骤去远程拷贝其余节点</span><br><span class="line"></span><br><span class="line">**3**. 搭建工作已经全部就绪，现在我们要来启动hadoop。如果有将hadoop的sbin和bin路径配置到环境变量PATH路径上则不用切换到如下路径：</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>cd /usr/local/hadoop/bin</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- 首先，我们要对namenode进行格式化：</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>hdfs namenode -format</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- 如果严格按照上述配置执行，那格式化namenode是不会失败的，如果失败，请到各个节点的tmp路径进行删除操作，然后重新格式化namenode。</span><br><span class="line"></span><br><span class="line">  切换到sbin路径上，启动hdfs:</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>cd /usr/local/hadoop/sbin</p>
<p>./start-dfs.sh</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">  启动成功之后，使用`jps`可以看到各个节点的进程。</span><br><span class="line"></span><br><span class="line">  hadoop1的进程，如图：</span><br><span class="line">  ![在这里插入图片描述](https://img-blog.csdnimg.cn/20190426151620386.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI1NTQyODc5,size_16,color_FFFFFF,t_70)</span><br><span class="line"></span><br><span class="line">  hadoop2的进程，如图：</span><br><span class="line">  ![在这里插入图片描述](https://img-blog.csdnimg.cn/20190426151629829.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI1NTQyODc5,size_16,color_FFFFFF,t_70)</span><br><span class="line"></span><br><span class="line">  hadoop3的进程，如图：</span><br><span class="line">  ![在这里插入图片描述](https://img-blog.csdnimg.cn/20190426151638234.png)</span><br><span class="line"></span><br><span class="line">  hadoop4的进程，如图：</span><br><span class="line">  ![在这里插入图片描述](https://img-blog.csdnimg.cn/20190426151646239.png)</span><br><span class="line"></span><br><span class="line">**4**.接下来，我们可以打开hadoop自带的管理也页面。在浏览器上输入：</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>hadoop1:50070</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">如图：</span><br><span class="line">![在这里插入图片描述](https://img-blog.csdnimg.cn/20190426151655743.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI1NTQyODc5,size_16,color_FFFFFF,t_70)</span><br><span class="line"></span><br><span class="line">想要检查是否4台机器已经做了集群也可以在上面看，如图红框部分显示4个存活节点：</span><br><span class="line">![在这里插入图片描述](https://img-blog.csdnimg.cn/20190426151706121.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI1NTQyODc5,size_16,color_FFFFFF,t_70)</span><br><span class="line"></span><br><span class="line">**5**.这还不算搭建完成，还需要启动yarn来进行任务调度，我们远程到hadoop2街店切换到sbin路径下：</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>ssh hadoop2<br>cd  /usr/local/hadoop/sbin<br>./start-yarn.sh</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">**启动成功之后，hadoop2的进程界面如图：**</span><br><span class="line">![在这里插入图片描述](https://img-blog.csdnimg.cn/20190426151717336.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI1NTQyODc5,size_16,color_FFFFFF,t_70)</span><br><span class="line"></span><br><span class="line">**在浏览器上输入：**</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>hadoop2:8088<br>```</p>
</li>
</ul>
<p>yarn的管理界面：<br><img src="https://img-blog.csdnimg.cn/20190426151729686.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI1NTQyODc5,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p><strong>6</strong>. 最后，<strong>hadoop搭建大功告成</strong>，接下来就可以进行hadoop命令进行文件操作和并行计算了。</p>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2021/01/15/Hadoop%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%EF%BC%88Hadoop3.3.0%EF%BC%89/" data-id="clh04c6dd0004o4rgct9w6imh" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2021/01/17/git%E7%AE%80%E5%8D%95%E5%91%BD%E4%BB%A4/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          (no title)
        
      </div>
    </a>
  
  
    <a href="/2021/01/12/linux%E7%B3%BB%E7%BB%9F%E9%85%8D%E7%BD%AE%E5%85%8D%E5%AF%86%E7%A0%81/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title"></div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/04/">April 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/03/">March 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/02/">February 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/01/">January 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/12/">December 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/10/">October 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/09/">September 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/05/">May 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/04/">April 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/02/">February 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/01/">January 2021</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2022/04/18/%E5%A4%8D%E4%B9%A0/">(no title)</a>
          </li>
        
          <li>
            <a href="/2022/03/23/git%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/">(no title)</a>
          </li>
        
          <li>
            <a href="/2022/03/11/Spring%20Security%E8%AE%A4%E8%AF%81%E4%B8%8E%E6%8E%88%E6%9D%83%E7%9A%84%E5%8E%9F%E7%90%86/">(no title)</a>
          </li>
        
          <li>
            <a href="/2022/02/16/java%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86%E5%AE%9E%E7%8E%B0%E4%B8%8E%E5%8E%9F%E7%90%86%E8%AF%A6%E7%BB%86%E5%88%86%E6%9E%90%20%EF%BC%88jdk%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86%EF%BC%89/">(no title)</a>
          </li>
        
          <li>
            <a href="/2022/01/06/Docker%20Volume/">(no title)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>